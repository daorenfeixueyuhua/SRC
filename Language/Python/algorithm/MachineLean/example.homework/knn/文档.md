
### **一、实验名称**
- 实验一：K近邻算法及应用

### **二、	实验目的及要求**  
- 掌握KNN算法的基本原理  
- 实现KNN算法
- 使用KNN算法解决一个具有应用问题  

### **三、	实验工具**
>百度Aistudio平台：

>Python版本：3.7.5；

>本实验中用到的python第三方模块：
> sklearn


### **四、	实验内容**

#### **1、算法基本基本原理**

>最简单最初级的分类器是将全部的训练数据所对应的类别都记录下来，
当测试对象的属性和某个训练对象的属性完全匹配时，便可以对其进行分类。
但是怎么可能所有测试对象都会找到与之完全匹配的训练对象呢，
其次就是存在一个测试对象同时与多个训练对象匹配，导致一个
训练对象被分到了多个类的问题，基于这些问题呢，就产生了KNN。

>KNN是通过测量不同特征值之间的距离进行分类。它的思路是：
如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的
大多数属于某一个类别，则该样本也属于这个类别，其中K通常是不大于20的整数。
KNN算法中，所选择的邻居都是已经正确分类的对象。该方法在定类决策上只依
据最邻近的一个或者几个样本的类别来决定待分样本所属的类别。

##### **KNN算法总结：**

>就是在训练集中数据和标签已知的情况下，输入测试数据，将测试数据的特征
与训练集中对应的特征进行相互比较，找到训练集中与之最为相似的前K个数据，
则该测试数据对应的类别就是K个数据中出现次数最多的那个分类，其算法的描述为：

- 计算测试数据与各个训练数据之间的距离；
- 按照距离的递增关系进行排序；
- 选取距离最小的K个点；
- 确定前K个点所在类别的出现频率；
- 返回前K个点中出现频率最高的类别作为测试数据的预测分类。

#### **2、核心代码**
```python
print('this is a test')
from sklearn import datasets
import math
import random
import operator


def loadDataset(split, trainingSet=[], testSet=[]):
    # with open(filename, 'r') as csvfile:
    #     lines = csv.reader(csvfile)
    dataset = datasets.load_iris().data
    for x in range(len(dataset) - 1):
        for y in range(4):
            dataset[x][y] = float(dataset[x][y])
        if random.random() < split:
            trainingSet.append(dataset[x])
        else:
            testSet.append(dataset[x])
    print('训练数据集: ', trainingSet)
    print('测试数据集: ', testSet)


def euclideanDistance(instance1, instance2, length):
    # 计算长度
    distance = 0
    for x in range(length):
        distance += pow((instance1[x] - instance2[x]), 2)
    return math.sqrt(distance)


def getAccuracy(testSet, predictions):
    # 计算准确率
    correct = 0
    for x in range(len(testSet)):
        if testSet[x][-1] == predictions[x]:
            correct += 1

    return (correct / float(len(testSet))) * 100.0


def getNeighbors(trainingSet, testInstance, k):
    distances = []
    length = len(testInstance) - 1
    for x in range(len(trainingSet)):
        # testinstance
        dist = euclideanDistance(testInstance, trainingSet[x], length)
        distances.append((trainingSet[x], dist))
        # distances.append(dist)
    distances.sort(key=operator.itemgetter(1))
    neighbors = []
    for x in range(k):
        neighbors.append(distances[x][0])
        return neighbors


def getResponse(neighbors):
    classVotes = {}
    for x in range(len(neighbors)):
        response = neighbors[x][-1]
        if response in classVotes:
            classVotes[response] += 1
        else:
            classVotes[response] = 1
        # 按降序pai lie
    sortedVotes = sorted(classVotes.items(), key=operator.itemgetter(1), reverse=True)
    return sortedVotes[0][0]


def main():
    # prepare data
    trainingSet = []
    testSet = []
    split = 0.67
    loadDataset(split, trainingSet, testSet)

    print("Train set: " + repr(len(trainingSet)))
    print('Test set: ' + repr(len(testSet)))
    # generate predictions
    predictions = []
    k = 3
    for x in range(len(testSet)):
        # trainingsettrainingSet[x]
        neighbors = getNeighbors(trainingSet, testSet[x], k)
        result = getResponse(neighbors)
        predictions.append(result)
        print('>predicted=' + repr(result) + ', actual=' + repr(testSet[x][-1]))
    accuracy = getAccuracy(testSet, predictions)
    print('Accuracy: ' + repr(accuracy) + '%')


if __name__ == '__main__':
    main()

```


### **五、	结论**
- （这里总结1-实验中用到的一些常用处理技巧或方法，如预处理方法等；2-归纳总结K近邻算法的优缺点以及适合/常用的应用场景；）

#### **2. 实验总结**

#### **5.2.1. KNN算法注意事项**

> K近邻算法的使用注意事项具体就是使用距离作为度量时，要保证所有特征在数值上是一个数量级上，以免距离的计算被数量级大的特征所主导。
> 在数据标准化这件事上，还要注意一点，训练数据集和测试数据集一定要使用同一标准的标准化。

#### **5.2.2. KNN算法优点**

- k近邻算法是一种在线技术，新数据可以直接加入数据集而不必进行重新训练
- k近邻算法理论简单，容易实现
- 准确性高，对异常值和噪声有较高的容忍度
- k近邻算法天生就支持多分类，区别与感知机、逻辑回归、SVM


#### **5.2.3. KNN算法缺点**

>K近邻算法的缺点，基本的 k近邻算法每预测一个“点”的分类都会重新进行一次全局运算，对于样本容量大的数据集计算量比较大。而且K近邻算法容易导致维度灾难，在高维空间中计算距离的时候，就会变得非常远；样本不平衡时，预测偏差比较大，k值大小的选择得依靠经验或者交叉验证得到。k的选择可以使用交叉验证，也可以使用网格搜索。k的值越大，模型的偏差越大，对噪声数据越不敏感，当 k的值很大的时候，可能造成模型欠拟合。k的值越小，模型的方差就会越大，当 k的值很小的时候，就会造成模型的过拟合。


#### **5.2.4. KNN算法使用场景**

> 由于KNN方法主要靠周围有限的邻近的样本，而不是靠判别类域的方法来确定所属类别的，因此对于类域的交叉或重叠较多的待分样本
>集来说，KNN方法较其他方法更为适合