爬虫简介：
	一段自动抓取互联网信息的程序
	价值：互联网数据，为我所用

简单爬虫架构
	爬虫调度端：url管理器->网页下载器->网页解析器
	运行流程

URL管理器
	防止重复抓取，防止循环抓取
	实现：
		内存：python set()集合
		关系数据库：MYSQL
		缓存数据库：Redis

网页下载器：
	将url对应的网页下载到本地
	urllib2
	requests

	urllib2:
		方法1：
			import urllib2
			response=urllib2.urlopen("http://baidu.com")
			print("response.getcode()")
			cont=response.read()
		方法2：添加data、http_header
			import urllib2
			request=urlib2.Request(url)
			request.add_data('a','1')
			request.add_header('User-Agent','Mozilla/5.0')
			response=urllib2.urlopen(request)
		方法3：添加特殊情景的处理器
			HTTPCookieProcessor	ProxyHandler	HTTPSHandler	HTTPRedirectHandler
			opener=urllib2.bulid_opener(handler)
			urllib2.install_opener(opener)
			urllib2.urlopen(url)
			urllib2.urlopen(request)

网页解析器
    从网页中提取有价的工具
    模糊解析：正则表达式
    结构化解析：DOM tree html.parse、Beatutiful Soup、lxml

    Beautiful soup
        Python第三方库，用于从HTML或XML中提取数据
        http://www.crummy.com/software/BeautifulSoup/

        安装：
            pip install beautifulsoup4
            import bs4

        语法：
            HTML网页：
                创建BeautifulSoup对象
                搜索节点find_all、find
                    按节点名称
                    按节点属性
                    按节点文字
                访问节点：名称、属性、文字
        实例爬虫
            确定目标
            分析目标※
                URL格式
                数据格式
                网页编码
            编写代码
            执行爬虫

            目标：百度百科Python词条相关词条页面-标题和简介
            入口页：http://baike.baidu.com/view/21087.html
            URL格式：
                词条页面URL:/view/125370.html
            数据格式：
                标题：
                    <dd class="lemmaWgt-lemmaTitle-title">
                        <h1>...</h1>
                    </dd>
                简介：
                    <div class="lemma-summary">***</div>
            页面编码：
                utf-8